          text: "承知しました。引き続きテキストでの回答を行います。"
        });
      }
      // どちらでもない場合は通常の処理を続行
    }
    
    // 特性分析に関連するメッセージかどうかを検出
    if (userMessage && (
      userMessage.includes('特性') || 
      userMessage.includes('分析') || 
      userMessage.includes('性格') || 
      userMessage.includes('過去の記録') || 
      userMessage.includes('履歴')
    )) {
      console.log(`\n======= 特性分析リクエスト検出 =======`);
      console.log(`→ ユーザーID: ${userId}`);
      console.log(`→ メッセージ: ${userMessage}`);
      console.log(`======= 特性分析リクエスト検出終了 =======\n`);
    }
    
    // Define feedback patterns for sentiment detection
    const FEEDBACK_PATTERNS = {
      positive: ['ありがとう', 'thank', 'thanks', 'good', 'helpful', 'useful', 'great', 'excellent', '助かる', '役に立つ', 'いいね', 'すごい', '素晴らしい'],
      negative: ['違う', 'wrong', 'bad', 'not helpful', 'useless', 'poor', 'terrible', '役に立たない', '違います', 'だめ', 'ダメ', '違いますよ', '違うよ']
    };
    
    // Check for general help request
    if (userMessage.toLowerCase() === 'ヘルプ' || 
        userMessage.toLowerCase() === 'help' || 
        userMessage.toLowerCase() === 'へるぷ') {
      // Return the general help message
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: helpSystem.getGeneralHelp()
      });
      return;
    }
    
    // Get user preferences to check for recently shown services
    const preferences = userPreferences.getUserPreferences(userId);
    
    // Check if this is a share mode message
    const { mode, limit } = determineModeAndLimit(userMessage);
    
    // シェアモードが判定された場合のLLM確認処理
    if (mode === 'share') {
      console.log(`Share mode triggered by determineModeAndLimit, confirming with LLM...`);
      const history = await fetchUserHistory(userId, 10);
      const isHighEngagement = await checkHighEngagement(userMessage, history);
      
      if (isHighEngagement) {
        console.log(`High engagement confirmed by LLM, sending sharing URL to user ${userId}`);
        // Send sharing message with Twitter URL
        await storeInteraction(userId, 'user', userMessage);
        const shareMessage = `お褒めの言葉をいただき、ありがとうございます！😊

Adamをお役立ていただけているようで、開発チーム一同とても嬉しく思います。もしよろしければ、下記のリンクからX(Twitter)でシェアしていただけると、より多くの方にAIカウンセラー「Adam」を知っていただけます。

${SHARE_URL}

通常の会話に戻る場合は、そのまま質問や相談を続けていただければと思います。`;

        await client.replyMessage(event.replyToken, {
          type: 'text',
          text: shareMessage
        });
        await storeInteraction(userId, 'assistant', shareMessage);
        return;
      } else {
        console.log(`LLM did not confirm high engagement despite keywords, processing as normal message`);
      }
    }
    
    // Track implicit feedback for recently shown services (continue with original handleText implementation)
    if (preferences && preferences.recentlyShownServices) {
      // Get services shown in the last hour
      const oneHourAgo = Date.now() - 3600000;
      let recentServices = [];
      
      // Collect service IDs shown in the last hour
      Object.entries(preferences.recentlyShownServices).forEach(([timestamp, services]) => {
        if (parseInt(timestamp) > oneHourAgo) {
          recentServices = [...recentServices, ...services];
        }
      });
      
      // If there are recent services, track implicit feedback
      if (recentServices.length > 0) {
        console.log(`Tracking implicit feedback for ${recentServices.length} recently shown services`);
        const feedbackResult = userPreferences.trackImplicitFeedback(userId, userMessage, recentServices);
        
        // If positive feedback was detected and preferences were updated, respond accordingly
        if (feedbackResult === true) {
          // Respond with a friendly acknowledgement
          return client.replyMessage(event.replyToken, {
            type: 'text',
            text: 'ありがとうございます！今後も役立つサービスをご紹介します。'
          });
        }
        
        // Clean up old entries
        const newRecentlyShownServices = {};
        Object.entries(preferences.recentlyShownServices).forEach(([timestamp, services]) => {
          if (parseInt(timestamp) > oneHourAgo) {
            newRecentlyShownServices[timestamp] = services;
          }
        });
        preferences.recentlyShownServices = newRecentlyShownServices;
        userPreferences.updateUserPreferences(userId, preferences);
      }
    }

    // Check for user preference commands
    const updatedPreferences = userPreferences.processPreferenceCommand(userId, userMessage);
    if (updatedPreferences) {
      let responseMessage = '';
      
      // Handle help request
      if (updatedPreferences.helpRequested) {
        responseMessage = userPreferences.getHelpMessage();
      } 
      // Handle settings check request
      else if (updatedPreferences.settingsRequested) {
        responseMessage = userPreferences.getCurrentSettingsMessage(userId);
      }
      // Handle preference updates
      else {
        // Create a more conversational response based on what was changed
        if (updatedPreferences.showServiceRecommendations !== undefined) {
          if (updatedPreferences.showServiceRecommendations) {
            // Check if this was triggered by positive feedback
            const lowerMessage = userMessage.toLowerCase();
            const isPositiveFeedback = FEEDBACK_PATTERNS.positive.some(pattern => lowerMessage.includes(pattern)) && 
                                      !FEEDBACK_PATTERNS.negative.some(pattern => lowerMessage.includes(pattern));
            
            if (isPositiveFeedback) {
              // Friendly response for positive feedback
              responseMessage = `ありがとうございます！今後も役立つサービスをご紹介します。`;
            } else {
              responseMessage = `サービス表示をオンにしました。お役立ちそうなサービスがあれば、会話の中でご紹介します。`;
            }
          } else {
            // Check if this was triggered by negative feedback
            const lowerMessage = userMessage.toLowerCase();
            const isNegativeFeedback = FEEDBACK_PATTERNS.negative.some(pattern => lowerMessage.includes(pattern));
            
            if (isNegativeFeedback) {
              // Minimal response for negative feedback
              responseMessage = `わかりました。`;
            } else {
              responseMessage = `サービス表示をオフにしました。`;
            }
          }
        } else if (updatedPreferences.maxRecommendations !== undefined) {
          if (updatedPreferences.maxRecommendations === 0) {
            responseMessage = `サービスを表示しない設定にしました。`;
          } else {
            responseMessage = `表示するサービスの数を${updatedPreferences.maxRecommendations}件に設定しました。`;
          }
        } else if (updatedPreferences.minConfidenceScore !== undefined) {
          responseMessage = `信頼度${Math.round(updatedPreferences.minConfidenceScore * 100)}%以上のサービスのみ表示するように設定しました。`;
        } else {
          // Fallback to current settings if we can't determine what changed
          responseMessage = userPreferences.getCurrentSettingsMessage(userId);
        }
      }
      
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: responseMessage
      });
      
      // Store the interaction
      await storeInteraction(userId, 'user', userMessage);
      await storeInteraction(userId, 'assistant', responseMessage);
      
      return;
    }
    
    // 特定の問い合わせ（ASD支援の質問例や使い方の案内）を検出
    if (userMessage.includes("ASD症支援であなたが対応できる具体的な質問例") && userMessage.includes("使い方")) {
      // Check if this user recently received an image generation - if so, skip ASD guide
      const recentImageTimestamp = recentImageGenerationUsers.get(userId);
      console.log(`[DEBUG] ASD Guide check - User ${userId} has recentImageTimestamp: ${recentImageTimestamp ? 'YES' : 'NO'}`);
      if (recentImageTimestamp) {
        const timeSinceImage = Date.now() - recentImageTimestamp;
        console.log(`[DEBUG] ASD Guide check - Time since image generation: ${timeSinceImage}ms, Protection threshold: 30000ms`);
      }
      
      if (recentImageTimestamp && (Date.now() - recentImageTimestamp < 30000)) { // 30 seconds protection
        console.log(`User ${userId} recently received image generation, skipping ASD guide`);
        recentImageGenerationUsers.delete(userId); // Clean up after use
        return;
      }
      
      return handleASDUsageInquiry(event);
    }
    
    // Check if image generation is in progress for this user - if so, skip further processing
    if (imageGenerationInProgress.has(userId)) {
      console.log(`Image generation in progress for ${userId}, skipping additional message handling`);
      return;
    }
    
    // pendingImageExplanations のチェック（はい/いいえ 判定）は冒頭で実施済み
    // 以下の重複するコードを削除
    /*
    if (pendingImageExplanations.has(userId)) {
      const pendingData = pendingImageExplanations.get(userId);
      const now = Date.now();
      if (pendingData.timestamp && (now - pendingData.timestamp > 5 * 60 * 1000)) { // 5分でタイムアウト
        console.log(`[DEBUG-IMAGE] Pending image request expired for ${userId} - ${Math.round((now - pendingData.timestamp)/1000)}s elapsed (max: 300s)`);
        pendingImageExplanations.delete(userId);
        // 通常の処理を続行
      } else if (userMessage === "はい") {
        console.log(`[DEBUG-IMAGE] 'はい' detected for user ${userId}, proceeding with image generation`);
        console.log(`[DEBUG-IMAGE] pendingData details: timestamp=${new Date(pendingData.timestamp).toISOString()}, contentLength=${pendingData.content ? pendingData.content.length : 0}`);
        
        // contentが存在するか確認
        if (!pendingData.content) {
          console.log(`[DEBUG-IMAGE] Error: pendingData.content is ${pendingData.content}`);
          await client.replyMessage(event.replyToken, {
            type: 'text',
            text: "申し訳ありません。画像生成に必要な情報が見つかりませんでした。もう一度お試しください。"
          });
          pendingImageExplanations.delete(userId);
          return;
        }
        
        const explanationText = pendingData.content;
        pendingImageExplanations.delete(userId);
        console.log(`[DEBUG-IMAGE] ユーザーの「はい」が検出されました。画像生成を開始します。内容: "${explanationText.substring(0, 30)}..."`);
        return handleVisionExplanation(event, explanationText);
      } else if (userMessage === "いいえ") {
        console.log(`[DEBUG-IMAGE] 'いいえ' detected for user ${userId}, cancelling image generation`);
        pendingImageExplanations.delete(userId);
        console.log(`[DEBUG-IMAGE] ユーザーの「いいえ」が検出されました。画像生成をキャンセルします。`);
        return client.replyMessage(event.replyToken, {
          type: 'text',
          text: "承知しました。引き続きテキストでの回答を行います。"
        });
      }
    }
    */

    // Add prevention check for users who just received image generation (prevents ASD guide sending)
    const recentImageTimestamp = recentImageGenerationUsers.get(userId);
    if (recentImageTimestamp) {
      const timeSinceImage = Date.now() - recentImageTimestamp;
      console.log(`[DEBUG] Recent image check - User ${userId}, time since image: ${timeSinceImage}ms, threshold: 10000ms`);
    }

    if (recentImageTimestamp && (Date.now() - recentImageTimestamp < 10000)) {
      console.log("画像生成直後のため、重複応答を防止します。");
      recentImageGenerationUsers.delete(userId);
      return;
    }

    // セキュリティチェック
    const isSafe = await securityFilterPrompt(userMessage);
    if (!isSafe) {
      const refusal = '申し訳ありません。このリクエストには対応できません。';
      await storeInteraction(userId, 'assistant', refusal);
      await client.replyMessage(event.replyToken, { type: 'text', text: refusal });
      return null;
    }

    // 最近の会話履歴の取得
    const historyData = await fetchUserHistory(userId, 10);
    const historyForProcessing = historyData.history || [];
    const historyMetadata = historyData.metadata || {};
    const systemPrompt = getSystemPromptForMode(mode);

    // 画像説明の提案トリガーチェック：isConfusionRequest のみを使用
    let triggerImageExplanation = false;
    
    // 拡張Embedding機能が利用可能な場合はそちらを使用
    if (global.enhancedImageDecision) {
      try {
        // 前の応答を取得（可能な場合）
        let previousResponse = null;
        if (historyForProcessing && historyForProcessing.length > 0) {
          for (let i = historyForProcessing.length - 1; i >= 0; i--) {
            if (historyForProcessing[i].role === 'assistant') {
              previousResponse = historyForProcessing[i].content;
              break;
            }
          }
        }
        
        // 拡張判定を使用
        const enhancedDecision = await global.enhancedImageDecision.shouldGenerateImage(userMessage, previousResponse);
        if (enhancedDecision) {
          console.log(`[DEBUG] Enhanced image generation decision triggered for: "${userMessage}"`);
          triggerImageExplanation = true;
        }
      } catch (error) {
        console.error('[ERROR] Enhanced image decision failed, falling back to standard method:', error.message);
        // 従来の方法にフォールバック
        if (isConfusionRequest(userMessage)) {
          console.log(`[DEBUG] Fallback: Direct image request detected in message: "${userMessage}"`);
          triggerImageExplanation = true;
        }
      }
    }
    // 拡張機能が利用できない場合は従来の方法を使用
    else if (isConfusionRequest(userMessage)) {
      console.log(`[DEBUG] Direct image request detected in message: "${userMessage}"`);
      triggerImageExplanation = true;
    }
    
    // それ以外のすべてのメッセージはLLMで分析
    if (!triggerImageExplanation) {
      try {
        console.log(`[DEBUG] Analyzing if user understands AI response: "${userMessage}"`);
        
        // 直前のAI回答を取得する
        // 会話履歴から直前のアシスタントメッセージを取得
        const lastAssistantMessage = historyForProcessing && historyForProcessing.length > 0 
          ? historyForProcessing.filter(item => item.role === 'assistant').pop() 
          : null;
        
        // lastAssistantMessageが未定義の場合、会話履歴から取得を試みる
        let previousAIResponse = null;
        
        if (lastAssistantMessage && lastAssistantMessage.content) {
          previousAIResponse = lastAssistantMessage.content;
          console.log(`[DEBUG-IMAGE] Using cached lastAssistantMessage: "${previousAIResponse.substring(0, 30)}..."`);
        } else if (historyForProcessing && historyForProcessing.length > 0) {
          // 会話履歴から最新のアシスタントメッセージを検索
          for (let i = historyForProcessing.length - 1; i >= 0; i--) {
            if (historyForProcessing[i].role === 'assistant') {
              previousAIResponse = historyForProcessing[i].content;
              console.log(`[DEBUG-IMAGE] Found assistant message in history: "${previousAIResponse.substring(0, 30)}..."`);
              break;
            }
          }
        }
        
        // 直前のAI回答がない場合はスキップ
        if (!previousAIResponse) {
          console.log(`[DEBUG-IMAGE] No previous AI response found in cache or history, skipping confusion detection`);
        } else {
          // OpenAI APIを使用して混乱度を判定
          const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
          
          const systemPrompt = `あなたはユーザーとAIの会話を分析し、ユーザーがAIの発言を理解できているかどうかを判断する専門家です。

あなたの任務は、「ユーザーがAIの直前の回答を理解していないかどうか」を判断することです。
ユーザーの発言から、AIの回答に対する混乱や理解困難が示されていると判断できる場合は、その確度（0〜100%）を評価してください。

確度が95%以上の場合のみ「CONFUSED:95」と回答し、それ以外は「NOT_CONFUSED」と回答してください。

判断の際は、以下のポイントに注意してください：
1. ユーザーの発言がAIの回答内容に関連しているか
2. ユーザーが「わからない」「理解できない」などの表現を使っているか
3. ユーザーが説明の簡略化や別の言い方を求めているか
4. ユーザーの質問がAIの回答内容を正しく理解していないことを示しているか

「億劫」などの表現は通常、混乱ではなく単に気が進まないという意味なので混乱とは判断しないでください。
純粋に会話を継続する意図の発言は混乱とみなさないでください。

回答は「CONFUSED:95」または「NOT_CONFUSED」の形式のみで返してください。`;

          // メッセージ配列を作成
          const messages = [
            { role: "system", content: systemPrompt },
            { role: "system", content: `直前のAIの回答: "${previousAIResponse.substring(0, 500)}${previousAIResponse.length > 500 ? '...' : ''}"` },
            { role: "user", content: userMessage }
          ];
          
          const response = await openai.chat.completions.create({
            model: "gpt-4o-mini",
            messages: messages,
            max_tokens: 10,
            temperature: 0.0
          });
          
          const content = response.choices[0].message.content.trim();
          console.log(`[DEBUG] LLM understanding analysis result: ${content}`);
          
          if (content.startsWith('CONFUSED:')) {
            const confidenceParts = content.split(':');
            if (confidenceParts.length > 1) {
              const confidence = parseFloat(confidenceParts[1]);
              if (confidence >= 95) {
                console.log(`[DEBUG] LLM determined user doesn't understand AI response with high confidence (${confidence}%)`);
                triggerImageExplanation = true;
              } else {
                console.log(`[DEBUG] LLM detected some confusion but confidence too low (${confidence}%)`);
              }
            }
          }
        }
      } catch (error) {
        console.error(`[DEBUG] Error in LLM understanding analysis: ${error.message}`);
      }
    }

    if (triggerImageExplanation) {
      console.log(`[DEBUG-IMAGE] Image explanation triggered for user ${userId}`);
      
      // Check if this user recently received an image generation - if so, skip image generation prompt
      const recentImageTimestamp = recentImageGenerationUsers.get(userId);
      if (recentImageTimestamp) {
        console.log(`[DEBUG-IMAGE] User ${userId} has recent image timestamp: ${recentImageTimestamp}, now: ${Date.now()}, diff: ${Date.now() - recentImageTimestamp}ms`);
      }
      
      if (recentImageTimestamp && (Date.now() - recentImageTimestamp < 30000)) { // 30 seconds protection
        console.log(`[DEBUG-IMAGE] User ${userId} recently received image generation, skipping image explanation offer`);
        recentImageGenerationUsers.delete(userId); // Clean up after use
        return;
      }
      
      // 会話履歴から直前のアシスタントメッセージを取得
      const lastAssistantMessage = historyForProcessing && historyForProcessing.length > 0 
        ? historyForProcessing.filter(item => item.role === 'assistant').pop() 
        : null;
      
      // lastAssistantMessageの取得を試みる（未定義の場合は履歴から取得）
      let contentToExplain = null;
      let contentSource = "unknown";
      
      if (lastAssistantMessage && lastAssistantMessage.content) {
        contentToExplain = lastAssistantMessage.content;
        contentSource = "cached";
        console.log(`[DEBUG-IMAGE] Using cached lastAssistantMessage for explanation: "${contentToExplain.substring(0, 30)}..."`);
      } else if (historyForProcessing && historyForProcessing.length > 0) {
        // 履歴から最新のアシスタントメッセージを検索
        for (let i = historyForProcessing.length - 1; i >= 0; i--) {
          if (historyForProcessing[i].role === 'assistant') {
            contentToExplain = historyForProcessing[i].content;
            contentSource = "history";
            console.log(`[DEBUG-IMAGE] Using message from history for explanation: "${contentToExplain.substring(0, 30)}..."`);
            break;
          }
        }
      }
      
      if (contentToExplain) {
        console.log(`[DEBUG-IMAGE] Setting pendingImageExplanations for user ${userId} with content from ${contentSource}: "${contentToExplain.substring(0, 30)}..."`);
        // タイムスタンプ付きで保存
        pendingImageExplanations.set(userId, {
          content: contentToExplain,
          timestamp: Date.now(),
          source: contentSource
        });
        // 画像生成提案状態をConversationHistoryに記録
        await storeInteraction(userId, 'system', `[画像生成提案] 提案時刻: ${new Date().toISOString()}, ソース: ${contentSource}`);
      } else {
        console.log(`[DEBUG-IMAGE] No content found for explanation, using default message`);
        pendingImageExplanations.set(userId, {
          content: "日常会話の基本とコミュニケーションのポイント",
          timestamp: Date.now(),
          source: "default"
        });
        // 画像生成提案状態をConversationHistoryに記録
        await storeInteraction(userId, 'system', `[画像生成提案] 提案時刻: ${new Date().toISOString()}, デフォルトテキスト使用`);
      }
      
      const suggestionMessage = "前回の回答について、画像による説明を生成しましょうか？「はい」または「いいえ」でお答えください。";
      console.log(`[DEBUG-IMAGE] 画像による説明の提案をユーザーに送信: "${suggestionMessage}"`);
      
      await storeInteraction(userId, 'assistant', suggestionMessage);
      return client.replyMessage(event.replyToken, {
        type: 'text',
        text: suggestionMessage
      });
    }

    // 通常のテキスト処理へ進む
    await storeInteraction(userId, 'user', userMessage);

    const historyForAIProcessing = await fetchUserHistory(userId, limit);
    // systemPrompt is already defined above

    // アドバイス要求の検出（非同期処理に対応）
    const adviceRequested = await detectAdviceRequestWithLLM(userMessage, historyForAIProcessing);
    
    // 会話履歴取得のデバッグログ
    console.log(`[会話履歴診断] ユーザー: ${userId}, モード: ${mode}, 取得履歴数: ${historyForAIProcessing.history?.length || 0}件`);
    
    // systemPrompt is already defined above
    
    // サービス表示の判断
    const showServices = await shouldShowServicesToday(userId, historyForAIProcessing, userMessage);

    // AIでの処理を実行
    const result = await processWithAI(systemPrompt, userMessage, historyForAIProcessing, mode, userId, client);
    
    // サービス推奨がある場合、それを応答に追加
    let finalResponse = result.response;
    
    // 応答が空の場合のフォールバック（LINE APIの400エラー防止）
    if (!finalResponse || finalResponse.trim() === '') {
      console.log(`⚠️ 警告: AIから空の応答を受け取りました。フォールバックメッセージを使用します。`);
      finalResponse = "申し訳ありません、処理中に問題が発生しました。しばらく経ってからもう一度お試しください。";
    }
    
    const serviceRecommendations = result.recommendations;
    
    if (serviceRecommendations && serviceRecommendations.length > 0) {
      console.log(`Adding ${serviceRecommendations.length} service recommendations to response`);
      
      // サービス推奨の表示用カテゴリを決定
      const category = mode === 'mental_health' ? 'mental_health' : 
                      mode === 'career' ? 'career' : 'general';
      
      // 自然な移行テキストを作成
      const transitionText = createNaturalTransition(finalResponse, category, false);
      
      // サービス情報を構築
      let serviceText = '';
      
      // 最大3つのサービスを表示
      const displayServices = serviceRecommendations.slice(0, 3);
      
      // サービス情報を追加
      displayServices.forEach((service, index) => {
        // サービス名の取得
        let serviceName;
        let serviceDescription = '';
        let serviceUrl = '';
        
        if (typeof service === 'string') {
          // サービスIDからサービス情報を取得
          const serviceInfo = servicesData.find(s => s.id === service);
          if (serviceInfo) {
            serviceName = serviceInfo.name;
            serviceDescription = serviceInfo.description;
            serviceUrl = serviceInfo.url;
          } else {
            serviceName = service;
          }
        } else if (service.name) {
          serviceName = service.name;
          serviceDescription = service.description || '';
          serviceUrl = service.url || '';
        } else if (service.serviceName) {
          serviceName = service.serviceName;
          serviceDescription = service.description || '';
          serviceUrl = service.url || '';
        } else if (service.id) {
          // サービスIDからサービス情報を取得
          const serviceInfo = servicesData.find(s => s.id === service.id);
          if (serviceInfo) {
            serviceName = serviceInfo.name;
            serviceDescription = serviceInfo.description;
            serviceUrl = serviceInfo.url;
          } else {
            serviceName = service.id;
          }
        }
        
        // サービス情報をテキストに追加
        serviceText += `${index + 1}. ${serviceName}`;
        if (serviceUrl) {
          serviceText += `\n   URL: ${serviceUrl}`;
        }
        if (serviceDescription) {
          // 説明文の切り捨て長さを80から150に拡大し、より自然な切り捨てを実現
          const maxDescLength = 150;
          let trimmedDesc = serviceDescription;
          if (serviceDescription.length > maxDescLength) {
            // 文の区切りで切る
            const lastSentenceEnd = serviceDescription.substring(0, maxDescLength).lastIndexOf('。');
            if (lastSentenceEnd > maxDescLength * 0.7) { // 70%以上の位置にある場合
              trimmedDesc = serviceDescription.substring(0, lastSentenceEnd + 1) + '...';
            } else {
              trimmedDesc = serviceDescription.substring(0, maxDescLength) + '...';
            }
          }
          serviceText += `\n   ${trimmedDesc}`;
        }
        serviceText += '\n\n'; // サービス間の区切りを改善
      });
      
      // 最終的な応答を構築
      finalResponse = `${finalResponse}${transitionText}${serviceText}`;
      
      // 推奨されたサービスを記録（将来のユーザーフィードバック追跡のため）
      const preferences = userPreferences.getUserPreferences(userId);
      if (preferences) {
        const timestamp = Date.now().toString();
        const serviceIds = displayServices.map(service => 
          typeof service === 'string' ? service : 
          service.id ? service.id : 
          service.serviceName ? service.serviceName : '');
          
        // 以前の表示済みサービス情報を読み込み
        preferences.recentlyShownServices = preferences.recentlyShownServices || {};
        
        // 新しい表示済みサービス情報を追加
        preferences.recentlyShownServices[timestamp] = serviceIds;
        
        // ユーザー設定を更新
        userPreferences.updateUserPreferences(userId, preferences);
      }
    }
    
    // サービス推奨が表示されない理由をユーザーに通知するための関数
    function getServiceNotificationMessage(userId, showServiceReason) {
      // 通知メッセージ - 実際のUI表示には使用せず、内部的に記録のみ
      if (!showServiceReason) {
        return null; // 理由が指定されていない場合は何も表示しない
      }

      const userPrefs = userPreferences.getUserPreferences(userId);
      
      switch (showServiceReason) {
        case 'disabled':
          return '（現在サービス推奨は無効になっています。「サービス表示オン」と入力すると有効になります）';
        case 'no_request':
          return '（明示的なアドバイス要求がない場合、サービス推奨は表示されません。「アドバイスください」などと入力すると表示されます）';
        case 'daily_limit':
          return '（1日の推奨上限に達しました。明日以降に再度ご利用ください）';
        case 'cooldown':
          return '（最近サービスを推奨したため、しばらく表示を控えています。少し時間をおいてから再度お試しください）';
        default:
          return null;
      }
    }

    // デバッグ情報として表示されない理由を取得
    // (serviceNotificationReasonの初期化が不足していたため、ここで適切に設定)
    let serviceNotificationReason = null;
    
    // ユーザー設定を取得
    const userPrefs = userPreferences.getUserPreferences(userId);
    
    // サービス表示がオフの場合に理由を設定
    if (!showServices && userPrefs && !userPrefs.showServiceRecommendations) {
      serviceNotificationReason = 'disabled';
    } else if (!showServices && !adviceRequested) {
      serviceNotificationReason = 'no_request';
    }
    
    // サービス推奨の通知メッセージを取得
    const notificationMessage = getServiceNotificationMessage(userId, serviceNotificationReason);
    if (notificationMessage) {
      console.log('Service notification message (debug only):', notificationMessage);
      // デバッグモードの場合のみ、AIの応答に追記（本番環境では表示しない）
      if (process.env.DEBUG_MODE === 'true') {
        finalResponse += '\n\n' + notificationMessage;
      }
    }
    
    // LINEのリプライ処理
    console.log(`Replying to ${event.replyToken} with message: ${finalResponse.substring(0, 50)}...`);
    
    // メッセージが空でないことを確認
    if (!finalResponse || finalResponse.trim() === '') {
      finalResponse = "申し訳ありません、エラーが発生しました。後でもう一度お試しください。";
      console.error(`⚠ LINE送信直前に空のメッセージを検出。デフォルトメッセージに置き換えました。`);
    }
    
    try {
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: finalResponse
      });
    } catch (replyError) {
      console.error(`LINE応答送信エラー: ${replyError.message}`);
      console.error(`応答の長さ: ${finalResponse.length}文字`);
      
      // メッセージが長すぎる場合は分割して送信を試みる
      if (finalResponse.length > 5000) {
        try {
          const shortenedResponse = finalResponse.substring(0, 4000) + "\n\n(メッセージが長すぎるため省略されました)";
          await client.replyMessage(event.replyToken, {
            type: 'text',
            text: shortenedResponse
          });
          console.log(`長いメッセージを短縮して送信しました。`);
        } catch (fallbackError) {
          console.error(`短縮メッセージの送信にも失敗: ${fallbackError.message}`);
        }
      }
    }
  } catch (error) {
    console.error('Error handling text message:', error);
    return Promise.resolve(null);
  }
}

// サーバー起動設定
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Listening on port ${PORT}`);
  console.log(`Visit: http://localhost:${PORT} (if local)\n`);
});

/**
 * Checks if a message indicates user confusion or a request for explanation about an image
 * Now only checks for direct image analysis requests
 * @param {string} text - The message text to check
 * @return {boolean} - True if the message is a direct request for image analysis
 */
function isConfusionRequest(text) {
  // 直接的な画像分析リクエストかどうかを判断するだけの機能に変更
  // 互換性のために関数名は変更せず
  if (!text || typeof text !== 'string') return false;
  
  // 掘り下げモードのリクエストは除外
  if (isDeepExplorationRequest(text)) {
    return false;
  }
  
  // 画像生成リクエストをチェック
  const imageGenerationRequests = [
    '画像を生成', '画像を作成', '画像を作って', 'イメージを生成', 'イメージを作成', 'イメージを作って',
    '図を生成', '図を作成', '図を作って', '図解して', '図解を作成', '図解を生成',
    'ビジュアル化して', '視覚化して', '絵を描いて', '絵を生成', '絵を作成',
    '画像で説明', 'イメージで説明', '図で説明', '視覚的に説明',
    '画像にして', 'イラストを作成', 'イラストを生成', 'イラストを描いて'
  ];
  
  // 画像生成リクエストまたは画像分析リクエストの場合はtrueを返す
  return imageGenerationRequests.some(phrase => text.includes(phrase)) || isDirectImageAnalysisRequest(text);
}

/**
 * Handles vision explanation requests
 * @param {Object} event - The LINE event object
 * @return {Promise<void>}
 */
async function handleVisionExplanation(event, explanationText) {
  const userId = event.source.userId;
  
  // Mark this user as having image generation in progress
  imageGenerationInProgress.set(userId, true);
  console.log(`Starting image generation for user ${userId} - setting protection flag`);
  
  // 画像生成開始を記録
  await storeInteraction(userId, 'system', `[画像生成開始] ${new Date().toISOString()}`);
  
  try {
    // explanationTextが提供されている場合、それを使用して画像説明を生成
    if (explanationText) {
      // Check if this is a long text like the ASD guide and summarize if needed
      let displayText = explanationText;
      let enhancedPrompt = "";
      let isASDGuide = false;
      
      // If text is very long (like the ASD guide), create a summary version
      if (explanationText.length > 300) {
        console.log(`[DEBUG] Long text detected (${explanationText.length} chars), creating summary version`);
        
        // Check if it's the ASD guide
        if (explanationText.includes("ASD支援機能の使い方ガイド") || explanationText.includes("自閉症スペクトラム障害")) {
          isASDGuide = true;
          displayText = "ASD支援機能の活用方法";
          enhancedPrompt = "ASD（自閉症スペクトラム障害）支援の主要なポイントを簡潔に示した視覚的な図解。質問例（コミュニケーション、感覚過敏、社会場面などの対応）、基本的な使い方、注意点を含む。シンプルで分かりやすいインフォグラフィック形式。";
          console.log(`[DEBUG] ASD guide detected, using specialized summary and prompt`);
        } else {
          // For other long texts, extract the first sentence or first 100 chars
          displayText = explanationText.split('。')[0] + "。";
          if (displayText.length > 100) {
            displayText = displayText.substring(0, 97) + "...";
          }
          enhancedPrompt = `以下の内容の要点を視覚的に説明するイラスト: ${explanationText.substring(0, 500)}`;
        }
      } else if (explanationText.length <= 20) {
        // 短いテキストの場合は教育的なコンテキストを追加
        console.log(`[DEBUG] Short text detected (${explanationText.length} chars), adding educational context`);
        displayText = explanationText;
        enhancedPrompt = `「${explanationText}」についての教育的で分かりやすい図解。日常生活での応用例や基本概念を含む、明るく親しみやすいイラスト。`;
      } else {
        // For normal length text, use as is
        enhancedPrompt = `以下のテキストに基づいて詳細で、わかりやすいイラストを作成してください。テキスト: ${explanationText}`;
      }
      
      // Use a simple message for generation notification
      let generationMessage = isASDGuide 
        ? "ASD支援機能の主なポイントを視覚化しています。少々お待ちください..."
        : `「${displayText}」に基づく視覚的な説明を生成しています。少々お待ちください...`;
      
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: generationMessage
      });
      
      // DALL-Eを使用して画像を生成
      try {
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY
        });
        
        console.log(`[DEBUG] Using enhanced prompt: ${enhancedPrompt.substring(0, 100)}...`);
        
        const response = await openai.images.generate({
          model: "dall-e-3",
          prompt: enhancedPrompt,
          n: 1,
          size: "1024x1024",
          quality: "standard"
        });
        
        const imageUrl = response.data[0].url;
        
        // 生成された画像のURLを取得
        console.log(`Generated image URL: ${imageUrl}`);
        
        // Create a concise response message
        let responseMessage = "";
        if (isASDGuide) {
          responseMessage = "ASD支援機能の主なポイントをまとめた画像です。この視覚的な説明は理解の助けになりましたか？";
        } else {
          responseMessage = `「${displayText}」の要点を視覚化しました。この画像は参考になりましたか？`;
        }
        
        // 画像をLINEに送信
        await client.pushMessage(userId, [
          {
            type: 'image',
            originalContentUrl: imageUrl,
            previewImageUrl: imageUrl
          },
          {
            type: 'text',
            text: responseMessage
          }
        ]);
        
        // 生成した画像情報を保存 - Store only image reference with concise text
        let storageText = isASDGuide ? "ASD支援機能の視覚的ガイド" : displayText;
        await storeInteraction(userId, 'assistant', `[生成画像] ${storageText}`);
        await storeInteraction(userId, 'system', `[画像生成完了] ${new Date().toISOString()}`);
        
        // Add user to recent image generation tracking with timestamp to prevent ASD guide
        recentImageGenerationUsers.set(userId, Date.now());
        console.log(`[DEBUG] Setting recentImageGenerationUsers timestamp for user ${userId}: ${Date.now()}`);
        
        // Clear the image generation flag after a delay (5 seconds should be enough)
        setTimeout(() => {
          imageGenerationInProgress.delete(userId);
          console.log(`Cleared image generation flag for user ${userId} after successful generation`);
          console.log(`[DEBUG] Image generation protection status - imageGenerationInProgress: ${imageGenerationInProgress.has(userId) ? 'YES' : 'NO'}, recentImageGenerationUsers timestamp: ${recentImageGenerationUsers.get(userId)}`);
        }, 5000);
        
      } catch (error) {
        console.error('DALL-E画像生成エラー:', error);
        
        // エラーの種類に応じたメッセージを提供
        let errorMessage = '申し訳ありません。画像の生成中にエラーが発生しました。';
        
        // 安全システムによる拒否の場合
        if (error.code === 'content_policy_violation' || 
            (error.message && error.message.includes('safety system'))) {
          errorMessage = '申し訳ありません。このテキストから安全な画像を生成できませんでした。「日常会話のポイント」「コミュニケーションの基本」などの具体的なテーマで試してみてください。';
        } else {
          errorMessage += '別の表現で試してみてください。';
        }
        
        await client.pushMessage(userId, {
          type: 'text',
          text: errorMessage
        });
        
        // Also clear the flag in case of error
        imageGenerationInProgress.delete(userId);
        console.log(`Cleared image generation flag for user ${userId} due to error`);
      }
      
      return;
    }
    
    // explanationTextがない場合は通常の画像履歴検索処理を行う
    // Get user's recent history to find the last image
    const history = await fetchUserHistory(userId, 10);
    
    // Find the most recent image message
    const lastImageMessage = history
      .filter(item => item.content && item.content.includes('画像が送信されました'))
      .pop();
    
    if (!lastImageMessage) {
      // No recent image found
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: '最近の画像が見つかりませんでした。説明してほしい画像を送信してください。もし画像の説明を求めていない場合は、別の質問をお願いします。'
      });
      return;
    }

    // 処理中であることを通知
    await client.replyMessage(event.replyToken, {
      type: 'text',
      text: '画像を分析しています。少々お待ちください...'
    });

    // 画像メッセージIDを抽出
    const messageIdMatch = lastImageMessage.content.match(/\(ID: ([^)]+)\)/);
    const messageId = messageIdMatch ? messageIdMatch[1] : null;
    
    if (!messageId) {
      throw new Error('画像メッセージIDが見つかりませんでした');
    }
    
    console.log(`Using image message ID: ${messageId} for analysis`);

    // LINE APIを使用して画像コンテンツを取得
    const stream = await client.getMessageContent(messageId);
    
    // 画像データをバッファに変換
    const chunks = [];
    for await (const chunk of stream) {
      chunks.push(chunk);
    }
    const imageBuffer = Buffer.concat(chunks);
    
    // Base64エンコード
    const base64Image = imageBuffer.toString('base64');
    
    // OpenAI Vision APIに送信するリクエストを準備
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "user",
          content: [
            { type: "text", text: "この画像について詳しく説明してください。何が写っていて、どんな状況か、重要な詳細を教えてください。" },
            { 
              type: "image_url", 
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`
              }
            }
          ]
        }
      ],
      max_tokens: 500
    });
    
    const analysis = response.choices[0].message.content;
    console.log(`Image analysis completed for user ${userId}`);
    
    // ユーザーに分析結果を送信
    await client.pushMessage(userId, {
      type: 'text',
      text: analysis
    });
    
    // 会話履歴に画像分析を記録
    await storeInteraction(userId, 'assistant', `[画像分析] ${analysis}`);
    
  } catch (error) {
    console.error('Error in handleVisionExplanation:', error);
    
    // エラーメッセージを送信
    try {
      await client.pushMessage(userId, {
        type: 'text',
        text: '申し訳ありません。画像の分析中にエラーが発生しました: ' + error.message
      });
    } catch (replyError) {
      console.error('Error sending error reply:', replyError);
    }
  }
}

/**
 * Extracts relevant context from conversation history
 * @param {Array} history - Array of conversation history items
 * @param {string} userMessage - Current user message
 * @return {Object} - Extracted context information
 */
function extractConversationContext(history, userMessage) {
  // 互換性のために同期版も維持
  // 非同期バージョンを呼び出し、結果をキャッシュするが、即座にプレースホルダーを返す
  
  // 基本的なコンテキスト情報
  const context = {
    userInterests: null,
    userEmotion: 'neutral',
    emotionIntensity: 0,
    messageCount: history.length,
    recentTopics: []
  };
  
  // 非同期処理をバックグラウンドで開始
  extractConversationContextAsync(history, userMessage)
    .then(asyncResult => {
      // グローバルキャッシュに結果を格納（他の呼び出しで再利用できるように）
      if (!global.contextCache) {
        global.contextCache = new Map();
      }
      const cacheKey = getCacheKeyForContext(history, userMessage);
      global.contextCache.set(cacheKey, asyncResult);
    })
    .catch(error => {
      console.error('Error in async context extraction:', error);
    });
  
  // キャッシュが存在する場合はそれを使用
  if (global.contextCache) {
    const cacheKey = getCacheKeyForContext(history, userMessage);
    if (global.contextCache.has(cacheKey)) {
      return global.contextCache.get(cacheKey);
    }
  }
  
  // キャッシュがない場合は従来の同期メソッドを使用
  return extractConversationContextLegacy(history, userMessage);
}

/**
 * キャッシュキーを生成するヘルパー関数
 */
function getCacheKeyForContext(history, userMessage) {
  // 最新の数メッセージとユーザーメッセージからハッシュを生成
  const recentMessages = history.slice(-3).map(msg => msg.content).join('|');
  const textToHash = `${recentMessages}|${userMessage}`;
  
  return crypto.createHash('md5').update(textToHash).digest('hex');
}

/**
 * 会話の文脈を意味的に抽出する非同期関数
 * @param {Array} history - 会話履歴
 * @param {string} userMessage - 現在のユーザーメッセージ
 * @returns {Promise<Object>} - 抽出された文脈情報
 */
async function extractConversationContextAsync(history, userMessage) {
  try {
    // EmbeddingServiceのインスタンスを取得または初期化
    if (!global.embeddingService) {
      const EmbeddingService = require('./embeddingService');
      global.embeddingService = new EmbeddingService();
      await global.embeddingService.initialize();
    }
    
    // 基本的なコンテキスト情報
    const context = {
      userInterests: [],
      userEmotion: 'neutral',
      emotionIntensity: 0,
      messageCount: history.length,
      recentTopics: []
    };
    
    // 最近のメッセージを抽出（最大5件）
    const recentMessages = history.slice(-5);
    const recentUserMessages = recentMessages
      .filter(msg => msg.role === 'user')
      .map(msg => msg.content);
    
    // 現在のメッセージを含む
    const allUserMessages = [...recentUserMessages, userMessage];
    const combinedUserText = allUserMessages.join(' ');
    
    // テキストが短すぎる場合は従来の方法を使用
    if (combinedUserText.length < 20) {
      return extractConversationContextLegacy(history, userMessage);
    }
    
    // 1. 感情分析 - 感情カテゴリと例文のマッピング
    const emotionExamples = {
      positive: "とても嬉しいです。素晴らしい気分です。ありがとう。楽しいです。最高です。",
      negative: "悲しいです。辛いです。苦しいです。困っています。心配です。不安です。",
      neutral: "特に何も感じません。普通です。ふつうです。特に変わりません。"
    };
    
    // 各感情カテゴリとの類似度を計算
    const emotionScores = {};
    for (const [emotion, examples] of Object.entries(emotionExamples)) {
      emotionScores[emotion] = await global.embeddingService.getTextSimilarity(
        userMessage, 
        examples
      );
    }
    
    // 最も高いスコアの感情を選択
    const dominantEmotion = Object.entries(emotionScores)
      .sort((a, b) => b[1] - a[1])[0];
    
    context.userEmotion = dominantEmotion[0];
    context.emotionIntensity = Math.round(dominantEmotion[1] * 10) / 10; // 0〜1の範囲に正規化
    
    // 2. 興味関心の分析
    // 興味を示す例文
    const interestExample = "私の趣味は○○です。○○に興味があります。○○が好きです。○○が楽しいです。";
    
    // 各ユーザーメッセージで興味関心の分析
    for (const msg of allUserMessages) {
      // 文単位での分析
      const sentences = msg.split(/。|！|\.|!/).filter(s => s.length > 5);
      
      for (const sentence of sentences) {
        const interestSimilarity = await global.embeddingService.getTextSimilarity(
          sentence,
          interestExample
        );
        
        // 興味関心を表す文であれば（閾値0.7以上）追加
        if (interestSimilarity > 0.7) {
          context.userInterests.push(sentence);
        }
      }
    }
    
    // 重複を削除
    context.userInterests = [...new Set(context.userInterests)];
    
    // userInterestsが空の場合はnullに設定
    if (context.userInterests.length === 0) {
      context.userInterests = null;
    }
    
    // 3. トピック抽出 - 最新の会話から主要なトピックを抽出
    // 文全体を結合
    const allText = allUserMessages.join('. ');
    
    // 短い文章に分割
    const segments = allText.split(/。|！|\.|!/).filter(s => s.length > 5);
    
    // セグメントをユニークにして最新の3つを保持
    context.recentTopics = [...new Set(segments)].slice(-3);
    
    return context;
  } catch (error) {
    console.error('Error in semantic conversation context extraction:', error);
    // エラー時は従来のメソッドにフォールバック
    return extractConversationContextLegacy(history, userMessage);
  }
}

/**
 * 従来の実装によるコンテキスト抽出（フォールバック用）
 */
function extractConversationContextLegacy(history, userMessage) {
  try {
    // Extract recent topics from last 5 messages
    const recentMessages = history.slice(-5);
    
    // Extract user interests
    const userInterests = [];
    const interestKeywords = [
      '趣味', '好き', '興味', 'ホビー', '楽しい', '関心', 
      'すき', 'きょうみ', 'たのしい', 'かんしん'
    ];
    
    recentMessages.forEach(msg => {
      if (msg.role === 'user') {
        for (const keyword of interestKeywords) {
          if (msg.content.includes(keyword)) {
            // Extract the sentence containing the keyword
            const sentences = msg.content.split(/。|！|\.|!/).filter(s => s.includes(keyword));
            userInterests.push(...sentences);
          }
        }
      }
    });
    
    // Check for emotion indicators
    const emotions = {
      positive: 0,
      negative: 0,
      neutral: 1 // Default to slightly neutral
    };
    
    const positiveWords = [
      '嬉しい', '楽しい', '良い', '好き', '素晴らしい', 
      'うれしい', 'たのしい', 'よい', 'すき', 'すばらしい'
    ];
    
    const negativeWords = [
      '悲しい', '辛い', '苦しい', '嫌い', '心配', 
      'かなしい', 'つらい', 'くるしい', 'きらい', 'しんぱい'
    ];
    
    // Check current message for emotion words
    for (const word of positiveWords) {
      if (userMessage.includes(word)) emotions.positive++;
    }
    
    for (const word of negativeWords) {
      if (userMessage.includes(word)) emotions.negative++;
    }
    
    // Return the compiled context
    return {
      userInterests: userInterests.length > 0 ? userInterests : null,
      userEmotion: emotions.positive > emotions.negative ? 'positive' : 
                   emotions.negative > emotions.positive ? 'negative' : 'neutral',
      emotionIntensity: Math.max(emotions.positive, emotions.negative),
      messageCount: history.length,
      recentTopics: recentMessages
        .map(msg => msg.content)
        .join(' ')
        .split(/。|！|\.|!/)
        .filter(s => s.length > 5)
        .slice(-3)
    };
  } catch (error) {
    console.error('Error extracting conversation context:', error);
    // Return a minimal context object in case of error
    return {
      userEmotion: 'neutral',
      emotionIntensity: 0,
      messageCount: history.length
    };
  }
}

async function processUserMessage(userId, userMessage, history, initialMode = null) {
  try {
    // Start timer for overall processing
    const overallStartTime = Date.now();
    console.log(`\n==== PROCESSING USER MESSAGE (${new Date().toISOString()}) ====`);
    console.log(`User ID: ${userId}`);
    console.log(`Message: ${userMessage.substring(0, 50)}${userMessage.length > 50 ? '...' : ''}`);
    
    // Get user preferences
    // ... existing code ...
  } catch (error) {
    console.error('Error processing user message:', error);
    return {
      type: 'text',
      text: '申し訳ありません。メッセージの処理中にエラーが発生しました。もう一度お試しください。'
    };
  }
}

/**
 * ユーザー入力の検証と無害化
 * @param {string} input - ユーザーからの入力メッセージ
 * @returns {string} - 検証済みの入力メッセージ
 */
function sanitizeUserInput(input) {
  if (!input) return '';
  
  // 文字列でない場合は文字列に変換
  if (typeof input !== 'string') {
    input = String(input);
  }
  
  // 最大長の制限
  const MAX_INPUT_LENGTH = 2000;
  if (input.length > MAX_INPUT_LENGTH) {
    console.warn(`ユーザー入力が長すぎます (${input.length} > ${MAX_INPUT_LENGTH}). 切り詰めます。`);
    input = input.substring(0, MAX_INPUT_LENGTH);
  }
  
  // XSS対策 - xssライブラリを使用
  input = xss(input);
  
  // SQL Injection対策 - SQL関連のキーワードを検出して警告
  const SQL_PATTERN = /\b(SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|UNION|JOIN|WHERE|OR)\b/gi;
  if (SQL_PATTERN.test(input)) {
    console.warn('SQL Injectionの可能性があるユーザー入力を検出しました');
    // キーワードを置換
    input = input.replace(SQL_PATTERN, '***');
  }
  
  return input;
}

/**
 * Line UserIDの検証
 * @param {string} userId - LineのユーザーID
 * @returns {string|null} - 検証済みのユーザーIDまたはnull
 */
function validateUserId(userId) {
  if (!userId || typeof userId !== 'string') {
    console.error('不正なユーザーID形式:', userId);
    return null;
  }
  
  // Line UserIDの形式チェック (UUIDv4形式)
  const LINE_USERID_PATTERN = /^U[a-f0-9]{32}$/i;
  if (!LINE_USERID_PATTERN.test(userId)) {
    console.error('Line UserIDの形式が不正です:', userId);
    return null;
  }
  
  return userId;
}

// Add cleanup for the tracking map every hour
// Setup a cleanup interval for recentImageGenerationUsers
setInterval(() => {
  const now = Date.now();
  recentImageGenerationUsers.forEach((timestamp, userId) => {
    // Remove entries older than 1 hour
    if (now - timestamp > 3600000) {
      recentImageGenerationUsers.delete(userId);
    }
  });
}, 3600000); // Clean up every hour

// Export functions for use in other modules
module.exports = {
  fetchUserHistory
};

/**
 * 会話履歴から特性分析を行い、レスポンスを生成する関数
 * @param {Array} history - 会話履歴の配列
 * @returns {Promise<string>} - 分析結果のテキスト
 */
async function generateHistoryResponse(history) {
  try {
    console.log(`\n======= 特性分析詳細ログ =======`);
    
    // 会話履歴が空の場合
    if (!history || history.length === 0) {
      console.log(`→ 会話履歴なし: 空のhistoryオブジェクト`);
      return "会話履歴がありません。もう少し会話を続けると、あなたの特性について分析できるようになります。";
    }

    console.log(`→ 分析開始: ${history.length}件の会話レコード`);
    
    // 会話履歴からユーザーのメッセージのみを抽出
    const userMessages = history.filter(msg => msg.role === 'user').map(msg => msg.content);
    console.log(`→ ユーザーメッセージ抽出: ${userMessages.length}件`);
    
    // 分析に十分なデータがあるかどうかを確認（最低1件あれば分析を試みる）
    if (userMessages.length > 0) {
      console.log(`→ OpenAI API呼び出し準備完了`);
      console.log(`→ プロンプト付与: "たとえデータが少なくても、「過去の記録がない」などとは言わず、利用可能なデータから最大限の分析を行ってください"`);
      
      // OpenAI APIを使用して特性分析を実行
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      
      const response = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          {
            role: "system",
            content: `あなたは「Adam」という発達障害専門のカウンセラーです。ユーザーの過去ログを分析し、以下の観点から深い洞察を提供してください。

[分析の観点]
1. コミュニケーションパターン
   - 言葉遣いの特徴
   - 表現の一貫性
   - 感情表現の方法

2. 思考プロセス
   - 論理的思考の特徴
   - 問題解決アプローチ
   - 興味・関心の対象

3. 社会的相互作用
   - 対人関係での傾向
   - ストレス対処方法
   - コミュニケーション上の強み/課題

4. 感情と自己認識
   - 感情表現の特徴
   - 自己理解の程度
   - モチベーションの源泉

[出力形式]
- 日本語で簡潔に（200文字以内）
- 肯定的な側面を含める
- 改善提案あれば添える
- 断定的な診断は避ける（専門医に相談を推奨する）
- 「データが不足している」「分析できない」「記録が少ない」などの否定的な表現は避け、限られたデータからでも何らかの洞察を提供する
- 専門家への相談を推奨する

重要: たとえデータが少なくても、「過去の記録がない」「データが少ない」「これまでの記録が少ない」などの表現は絶対に使わず、利用可能なデータから最大限の具体的な分析を行ってください。データ量についての言及は一切避け、直接分析内容を伝えてください。`
          },
          {
            role: "user",
            content: `以下はユーザーの過去の会話履歴です。この情報を基に、ユーザーの特性について分析してください。
            
会話履歴:
${userMessages.join('\n\n')}`
          }
        ],
        max_tokens: 500
      });
      
      console.log(`→ OpenAI API応答受信: ${response.choices[0].message.content.substring(0, 50)}...`);
      console.log(`→ レスポンスが「過去の記録がない」を含むか: ${response.choices[0].message.content.includes('過去の記録がない') || response.choices[0].message.content.includes('会話履歴がない')}`);
      console.log(`======= 特性分析詳細ログ終了 =======\n`);
      return response.choices[0].message.content;
    } else {
      console.log(`→ 分析に利用可能なメッセージなし`);
      console.log(`======= 特性分析詳細ログ終了 =======\n`);
      // 会話履歴が不足している場合でも、否定的な表現は避ける
      return "会話履歴を分析しました。より詳細な特性分析のためには、もう少し会話を続けることをお勧めします。現時点では、あなたの興味や関心に合わせたサポートを提供できるよう努めています。何か具体的な質問や話題があれば、お気軽にお聞かせください。";
    }
  } catch (error) {
    console.error('Error in generateHistoryResponse:', error);
    console.error(`→ エラースタックトレース: ${error.stack}`);
    console.log(`======= 特性分析詳細ログ終了 (エラー発生) =======\n`);
    // エラーが発生した場合でも、ユーザーフレンドリーなメッセージを返す
    return "申し訳ありません。特性分析の処理中にエラーが発生しました。もう一度お試しいただくか、別の質問をしていただけますか？";
  }
}

/**
 * 混乱や理解困難を示す表現を含むかどうかをチェックする
 * @param {string} text - チェックするテキスト
 * @return {boolean} - 混乱表現を含む場合はtrue
 */
function containsConfusionTerms(text) {
  if (!text || typeof text !== 'string') return false;
  
  // 一般的な混乱表現
  const confusionTerms = [
    'わからない', '分からない', '理解できない', '意味がわからない', '意味が分からない',
    'どういう意味', 'どういうこと', 'よくわからない', 'よく分からない',
    '何が言いたい', 'なにが言いたい', '何を言ってる', 'なにを言ってる',
    'もう少し', 'もっと', '簡単に', 'かみ砕いて', 'シンプルに', '例を挙げて',
    '違う方法で', '別の言い方', '言い換えると', '言い換えれば', '詳しく',
    '混乱', '複雑', '難解', 'むずかしい'
  ];
  
  return confusionTerms.some(term => text.includes(term));
}

/**
 * 直接的な画像分析リクエストかどうかを判断する
 * @param {string} text - チェックするテキスト
 * @return {boolean} - 直接的な画像分析リクエストの場合はtrue
 */
function isDirectImageAnalysisRequest(text) {
  if (!text || typeof text !== 'string') return false;
  
  // 画像分析に特化したフレーズ
  const directAnalysisRequests = [
    'この画像について', 'この写真について', 'この画像を分析', 'この写真を分析',
    'この画像を解析', 'この写真を解析', 'この画像を説明', 'この写真を説明',
    'この画像の内容', 'この写真の内容', 'この画像に写っているもの', 'この写真に写っているもの'
  ];
  
  // 直接的な画像分析リクエストの場合はtrueを返す
  return directAnalysisRequests.some(phrase => text.includes(phrase));
}

// 定数宣言の部分の後に追加
const PENDING_IMAGE_TIMEOUT = 5 * 60 * 1000; // 5分のタイムアウト

// server.js内の起動処理部分（通常はexpressアプリの初期化後）に追加
// アプリケーション起動時にシステムステートを復元する関数
async function restoreSystemState() {
  try {
    console.log('Restoring system state from persistent storage...');
    
    // 保留中の画像生成リクエストの復元
    await restorePendingImageRequests();
    
    console.log('System state restoration completed');
  } catch (error) {
    console.error('Error restoring system state:', error);
  }
}

// 会話履歴から保留中の画像生成リクエストを復元する関数
async function restorePendingImageRequests() {
  try {
    console.log('Attempting to restore pending image generation requests...');
    
    if (!process.env.AIRTABLE_API_KEY || !process.env.AIRTABLE_BASE_ID) {
      console.log('Airtable credentials not found. Cannot restore pending image requests.');
      return;
    }
    
    // グローバル変数のairtableBaseを使用
    if (!airtableBase) {
      console.error('Airtable connection not initialized. Cannot restore pending image requests.');
      return;
    }
    
    // 最近の画像生成提案を検索（過去30分以内）
    const cutoffTime = new Date(Date.now() - 30 * 60 * 1000); // 30分前
    const cutoffTimeStr = cutoffTime.toISOString();
    
    const pendingProposals = await airtableBase('ConversationHistory')
      .select({
        filterByFormula: `AND(SEARCH("[画像生成提案]", {Content}) > 0, {Timestamp} > "${cutoffTimeStr}")`,
        sort: [{ field: 'Timestamp', direction: 'desc' }]
      })
      .firstPage();
    
    console.log(`Found ${pendingProposals.length} recent image generation proposals`);
    
    // 各提案についてユーザーの応答をチェック
    for (const proposal of pendingProposals) {
      const userId = proposal.get('UserID');
      const proposalTime = new Date(proposal.get('Timestamp')).getTime();
      const now = Date.now();
      
      // タイムアウトチェック
      if (now - proposalTime > PENDING_IMAGE_TIMEOUT) {
        console.log(`Skipping expired proposal for user ${userId} (${Math.round((now - proposalTime)/1000)}s old)`);
        continue;
      }
      
      // 提案後のユーザー応答を確認
      const userResponses = await airtableBase('ConversationHistory')
        .select({
          filterByFormula: `AND({UserID} = "${userId}", {Role} = "user", {Timestamp} > "${proposal.get('Timestamp')}")`,
          sort: [{ field: 'Timestamp', direction: 'asc' }]
        })
        .firstPage();
      
      console.log(`[DEBUG-RESTORE] User ${userId}: proposal time=${new Date(proposalTime).toISOString()}, found ${userResponses.length} responses after proposal`);
      
      // ユーザーが応答していない場合、提案を保留中として復元
      if (userResponses.length === 0) {
        console.log(`[DEBUG-RESTORE] Restoring pending image proposal for user ${userId} - no responses found after proposal`);
        
        // 最後のアシスタントメッセージを取得（提案の直前のメッセージ）
        const lastMessages = await airtableBase('ConversationHistory')
          .select({
            filterByFormula: `AND({UserID} = "${userId}", {Role} = "assistant", {Timestamp} < "${proposal.get('Timestamp')}")`,
            sort: [{ field: 'Timestamp', direction: 'desc' }],
            maxRecords: 1
          })
          .firstPage();
        
        if (lastMessages.length > 0) {
          const content = lastMessages[0].get('Content');
          pendingImageExplanations.set(userId, {
            content: content,
            timestamp: proposalTime
          });
          console.log(`[DEBUG-RESTORE] Restored pending image explanation for user ${userId} with content: "${content.substring(0, 30)}..." at timestamp ${new Date(proposalTime).toISOString()}`);
        } else {
          console.log(`[DEBUG-RESTORE] Could not find assistant message before proposal for user ${userId}`);
        }
      } else {
        console.log(`[DEBUG-RESTORE] User ${userId} already responded after proposal, not restoring`);
        if (userResponses.length > 0) {
          console.log(`[DEBUG-RESTORE] First response: "${userResponses[0].get('Content')}" at ${userResponses[0].get('Timestamp')}`);
        }
      }
    }
    
    // 復元された内容の詳細なデバッグ情報
    if (pendingImageExplanations.size > 0) {
      console.log('=== Restored pending image requests details ===');
      for (const [uid, data] of pendingImageExplanations.entries()) {
        console.log(`User ${uid}: timestamp=${new Date(data.timestamp).toISOString()}, age=${Math.round((Date.now() - data.timestamp)/1000)}s, contentLen=${data.content.length}`);
        console.log(`Content preview: "${data.content.substring(0, 30)}..."`);
      }
      console.log('============================================');
    } else {
      console.log('No valid pending image requests were found to restore');
    }
    
    console.log(`Successfully restored ${pendingImageExplanations.size} pending image requests`);
  } catch (error) {
    console.error('Error restoring pending image requests:', error);
  }
}

// アプリケーション起動時に状態を復元
restoreSystemState();

/**
 * Use GPT-4o-mini to determine if user is asking for advice or in need of service recommendations
 */
async function detectAdviceRequestWithLLM(userMessage, history) {
  try {
    console.log('Using LLM to analyze if user needs service recommendations');
    
    const prompt = `
ユーザーの次のメッセージから、アドバイスやサービスの推薦を求めているか、または困った状況にあるかを判断してください:

"${userMessage}"

判断基準:
1. ユーザーが明示的にアドバイスやサービスの推薦を求めている
2. ユーザーが困った状況や問題を抱えており、サービス推薦が役立つ可能性がある
3. 単なる雑談やお礼の場合は推薦不要
4. ユーザーが推薦を拒否している場合は推薦不要

応答は「yes」または「no」のみで答えてください。
`;

    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: "あなたはユーザーの意図を正確に判断するAIです。yes/noのみで回答してください。" },
        { role: "user", content: prompt }
      ],
      temperature: 0.1,
      max_tokens: 10
    });
    
    const result = response.choices[0].message.content.trim().toLowerCase();
    
    // 詳細なログを追加
    if (result === 'yes') {
      console.log(`✅ Advice request detected by LLM: "${userMessage.substring(0, 50)}${userMessage.length > 50 ? '...' : ''}"`);
    } else {
      console.log(`❌ No advice request detected by LLM: "${userMessage.substring(0, 50)}${userMessage.length > 50 ? '...' : ''}"`);
    }
    
    return result === 'yes';
  } catch (error) {
    console.error('Error in LLM advice request detection:', error);
    // Fall back to simpler heuristic in case of error
    console.log(`⚠️ Error in advice request detection, defaulting to false`);
    return false;
  }
}

/**
 * [新機能] 拡張Embedding機能への橋渡し
 * 既存の機能を変更せず、機能を追加するための関数
 * global.detectAdviceRequestWithLLMへの参照を設定
 */
// グローバルに関数を公開（他モジュールからのアクセス用）
global.detectAdviceRequestWithLLM = detectAdviceRequestWithLLM;
global.isConfusionRequest = isConfusionRequest;
global.isDeepExplorationRequest = isDeepExplorationRequest;

// 拡張機能のサポート用ヘルパー（初期化が済んでいない場合に安全に実行）
const initializeEmbeddingBridge = async () => {
  try {
    // サービスマッチング機能の初期化と組み込み
    if (typeof enhancedServiceMatching === 'undefined' && fs.existsSync('./enhancedServiceMatching.js')) {
      global.enhancedServiceMatching = require('./enhancedServiceMatching');
      await global.enhancedServiceMatching.initialize();
      console.log('Enhanced service matching bridge initialized successfully');
    }
    
    // 画像判断機能の初期化と組み込み
    if (typeof enhancedImageDecision === 'undefined' && fs.existsSync('./enhancedImageDecision.js')) {
      global.enhancedImageDecision = require('./enhancedImageDecision');
      await global.enhancedImageDecision.initialize();
      console.log('Enhanced image decision bridge initialized successfully');
    }
  } catch (error) {
    console.error('Error initializing embedding bridges:', error);
  }
};

// 非同期で拡張機能を初期化（サーバー起動を遅延させない）
setTimeout(initializeEmbeddingBridge, 2000);

/**
 * Check if it's an appropriate time in the conversation to show service recommendations
 */
async function shouldShowServicesToday(userId, history, userMessage) {
  // 拡張機能が利用可能な場合はそちらを使用
  if (global.enhancedServiceMatching) {
    try {
      const enhancedDecision = await global.enhancedServiceMatching.shouldShowServiceRecommendation(
        userMessage, 
        history, 
        userId
      );
      console.log(`[DEBUG] Enhanced service recommendation decision: ${enhancedDecision}`);
      return enhancedDecision;
    } catch (error) {
      console.error('[ERROR] Enhanced service recommendation failed, falling back to standard method:', error.message);
      // 従来の方法にフォールバック
    }
  }
  
  // If user explicitly asks for advice/services, always show
  const isAdviceRequest = await detectAdviceRequestWithLLM(userMessage, history);
  if (isAdviceRequest) {
    console.log('✅ Advice request detected by LLM in shouldShowServicesToday - always showing services');
    return true;
  }
  
  try {
    // Use a shared function to get/set last service time
    const userPrefs = userPreferences.getUserPreferences(userId);
    const lastServiceTime = userPrefs.lastServiceTime || 0;
    const now = Date.now();
    
    // If user recently received service recommendations (within last 4 hours)
    if (lastServiceTime > 0 && now - lastServiceTime < 4 * 60 * 60 * 1000) {
      // Count total service recommendations today
      const todayStart = new Date();
      todayStart.setHours(0, 0, 0, 0);
      
      let servicesToday = 0;
      if (userPrefs.recentlyShownServices) {
        for (const timestamp in userPrefs.recentlyShownServices) {
          if (parseInt(timestamp) > todayStart.getTime()) {
            servicesToday += userPrefs.recentlyShownServices[timestamp].length;
          }
        }
      }
      
      // Limit to no more than 9 service recommendations per day
      if (servicesToday >= 9) {
        console.log('⚠️ Daily service recommendation limit reached (9 per day) - not showing services');
        return false;
      }
      
      // If fewer than 5 service recommendations today, require a longer minimum gap
      if (servicesToday < 5 && now - lastServiceTime < 45 * 60 * 1000) {
        console.log(`⚠️ Time between service recommendations too short (< 45 minutes) - not showing services. Last shown: ${Math.round((now - lastServiceTime) / 60000)} minutes ago`);
        return false; // Less than 45 minutes since last recommendation
      }
    }

    return true;
  } catch (err) {
    console.error('Error in shouldShowServicesToday:', err);
    return true; // Default to showing if there's an error
  }
}

/**
 * Safety check for images using OpenAI's moderation capability with GPT-4o-mini
 * @param {string} base64Image - Base64 encoded image
 * @return {Promise<boolean>} - Whether the image passed the safety check
 */
async function checkImageSafety(base64Image) {
  try {
    // Using OpenAI's GPT-4o-mini model to detect potential safety issues
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: "あなたは画像モデレーターです。この画像が安全かどうかを判断してください。画像が暴力的、性的、または不適切な内容が含まれている場合、それを特定してください。回答は「SAFE」または「UNSAFE」で始めてください。"
        },
        {
          role: "user",
          content: [
            { 
              type: "image_url", 
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`
              }
            }
          ]
        }
      ],
      max_tokens: 150,
      temperature: 0
    });
    
    const moderationResult = response.choices[0].message.content;
    console.log(`Image safety check (4o-mini): ${moderationResult}`);
    
    // If the response starts with UNSAFE, the image didn't pass the safety check
    return !moderationResult.startsWith("UNSAFE");
  } catch (error) {
    console.error('Error in image safety check:', error);
    // In case of error, assume the image is safe to not block valid images
    return true;
  }
}

// Near the end of the file where global.isConfusionRequest is defined

// Make isDeepExplorationRequest globally accessible
global.isDeepExplorationRequest = isDeepExplorationRequest;

// Export functions to make them available to other modules
module.exports = {
  // ... existing exports ...
  isConfusionRequest,
  isDeepExplorationRequest,
  // ... existing exports ...
};
