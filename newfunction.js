async function isJobRequestSemantic(text) {
  // Skip semantic analysis for obvious cases
  if (text.includes('é©è·') || text.includes('ã‚­ãƒ£ãƒªã‚¢è¨ºæ–­') || text.includes('å‘ã„ã¦ã‚‹ä»•äº‹') || 
      (text.includes('æ€ã„å‡ºã—ã¦') && (text.includes('é©è·') || text.includes('ä»•äº‹') || text.includes('ã‚­ãƒ£ãƒªã‚¢'))) ||
      /è¨˜éŒ².*(æ€ã„å‡º|æ•™ãˆ|è¨ºæ–­).*(é©è·|ä»•äº‹|è·æ¥­|ã‚­ãƒ£ãƒªã‚¢)/.test(text)) {
    console.log('ğŸ‘” ã‚­ãƒ£ãƒªã‚¢æ¤œå‡º: æ˜ç¤ºçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡º: ' + text.substring(0, 30));
    return true;
  }
  
  try {
    console.log('ğŸ§  ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡º: åˆ†æé–‹å§‹: ' + text.substring(0, 30));
    
    const prompt = ;

    const response = await openai.chat.completions.create({
      model: "o3-mini-2025-01-31", // Use a small, fast model for classification
      messages: [
        { role: "system", content: "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ„å›³ã‚’æ­£ç¢ºã«åˆ¤æ–­ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚" },
        { role: "user", content: prompt }
      ],
      temperature: 0,
      max_tokens: 5, // Just need YES or NO
    });

    const decision = response.choices[0].message.content.trim();
    const isCareerRequest = decision.includes("YES");
    
    console.log('ğŸ§  ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡º: çµæœ: ' + (isCareerRequest ? "ã‚­ãƒ£ãƒªã‚¢é–¢é€£" : "ã‚­ãƒ£ãƒªã‚¢ä»¥å¤–") + ', ãƒ¢ãƒ‡ãƒ«å›ç­”: "' + decision + '"');
    
    return isCareerRequest;
  } catch (error) {
    console.error('âŒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡ºã‚¨ãƒ©ãƒ¼: ' + error.message);
    // Fall back to the pattern matching approach on error
    return isJobRequest(text);
  }
}
